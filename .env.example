# ==============================================================================
# ||        LLM API Key Proxy - Environment Variable Configuration        ||
# ==============================================================================
#
#
# This file provides an example configuration for proxy server.
# Copy this file to a new file named '.env' in the same directory.
# and replace placeholder values with your actual credentials and settings.
# ------------------------------------------------------------------------------
# | [REQUIRED] Proxy Server Settings                                           |
# ------------------------------------------------------------------------------
#
# [REQUIRED] A secret key used to authenticate requests to THIS proxy server.
# This can be any string. Your client application must send this key in
#
# Example: PROXY_API_KEY="sk-1234567890abcdefg"
#             PROXY_API_KEY="YOUR_PROXY_API_KEY"
#
#
# ------------------------------------------------------------------------------
# | [SECURITY] CORS Configuration |
# ------------------------------------------------------------------------------
#
# CORS_ORIGINS: Comma-separated list of allowed origins (e.g., "https://app.example.com,https://app2.example.com")
# Default: Empty (no CORS - most secure, only same-origin requests allowed)
# To allow all origins (INSECURE): CORS_ORIGINS=*
# Note: When allowing all origins, credentials will be disabled automatically
#
# CORS_ORIGINS="https://your-frontend-app.com"
#

# ------------------------------------------------------------------------------
# | [API KEYS] Provider API Keys                                               |
# ------------------------------------------------------------------------------
#
# The proxy automatically discovers API keys from environment variables.
# To add multiple keys for a single provider, increment the number at the end of
# of the variable name (e.g., GEMINI_API_KEY_1, GEMINI_API_KEY_2).
#
# Valid levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
# LOG_LEVEL=INFO
# LOG_FILE=app.log
#
# ------------------------------------------------------------------------------
# --- Cloudflare Workers AI (10,000 Neurons/day Free) ---
# Sign up: https://workers.cloudflare.com/workers-ai
# Get Account ID: https://dash.cloudflare.com/
# Create API Token: https://dash.cloudflare.com/profile/api-tokens (Workers AI edit permission)
# Pricing: Free tier 10,000 Neurons/day, $0.011/1000 Neurons after
#
# CLOUDFLARE_ACCOUNT_ID=your-account-id
# CLOUDFLARE_API_TOKEN=your-api-token
#
# ------------------------------------------------------------------------------
# --- Puter.js Provider (FREE 500+ Models) ---
# Puter.js provides free access to 500+ models via the User-Pays model.
# Models include: GPT-5, GPT-4o, Claude Sonnet 4.5, Claude Opus 4.5, Gemini 2.5, DeepSeek V3/R1, etc.
# API wrapper: https://puter-free-chatbot.vercel.app/api
#
# Usage: Unlimited (no rate limits for developers)
# Priority: Tier 5 (Experimental/Unstable - currently returning 403s)
#
PUTER_API_BASE="https://puter-free-chatbot.vercel.app/api"
PROVIDER_PRIORITY_PUTER=5

# ------------------------------------------------------------------------------
# --- G4F Fallback Configuration ---
# API Key for G4F (if using a protected instance)
G4F_API_KEY=
# Base URL for G4F (optional, defaults to internal)
G4F_MAIN_API_BASE=
# Priority for G4F (Higher number = Lower priority)
# Default for paid providers is usually 1 or 2.
PROVIDER_PRIORITY_G4F=5
# The provider name is derived from the part of the variable name before "_API_KEY".
# For example, 'GEMINI_API_KEY_1' configures the 'gemini' provider.
#

# --- Google Gemini ---
GEMINI_API_KEY_1="YOUR_GEMINI_API_KEY_1"
GEMINI_API_KEY_2="YOUR_GEMINI_API_KEY_2"

# --- OpenAI / Azure OpenAI ---
# For Azure, ensure your key has access to the desired models.
OPENAI_API_KEY_1="YOUR_OPENAI_OR_AZURE_API_KEY"

# --- Anthropic (Claude) ---
ANTHROPIC_API_KEY_1="YOUR_ANTHROPIC_API_KEY"

# --- OpenRouter ---
OPENROUTER_API_KEY_1="YOUR_OPENROUTER_API_KEY"

# --- Groq ---
GROQ_API_KEY_1="YOUR_GROQ_API_KEY"

# --- Mistral AI ---
MISTRAL_API_KEY_1="YOUR_MISTRAL_API_KEY"

# --- NVIDIA NIM ---
NVIDIA_API_KEY_1="YOUR_NVIDIA_API_KEY"

# --- Co:here ---
COHERE_API_KEY_1="YOUR_COHERE_API_KEY"

# --- AWS Bedrock ---
# Note: Bedrock authentication is typically handled via AWS IAM roles or
# environment variables like AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.
# Only set this if you are using a specific API key for Bedrock.
BEDROCK_API_KEY_1=""

# --- Chutes ---
CHUTES_API_KEY_1="YOUR_CHUTES_API_KEY"

# --- AgentRouter ---
# OpenAI-compatible API with various models (DeepSeek, GLM, etc.)
# Get your API key at: https://agentrouter.org/console/token
# API Base: https://agentrouter.org/v1
AGENTROUTER_API_KEY_1="YOUR_AGENTROUTER_API_KEY"

# --- Cerebras ---
# Extremely fast inference with generous free tier (1M tokens/day)
# Get your API key at: https://cloud.cerebras.ai
# API Base: https://api.cerebras.ai/v1
# Models: llama-3.1-8b, llama-3.3-70b, qwen-3-32b, qwen-3-235b
CEREBRAS_API_KEY_1="YOUR_CEREBRAS_API_KEY"

# --- HuggingFace Inference API ---
# Free inference for open-source models (queue-based, may wait if busy)
# Get your API token at: https://huggingface.co/settings/tokens
# API Base: https://api-inference.huggingface.co/models
# Models: Qwen/Qwen2.5-72B-Instruct, meta-llama/Llama-3.3-70B-Instruct, etc.
HUGGINGFACE_API_KEY_1="YOUR_HUGGINGFACE_API_TOKEN"

# ------------------------------------------------------------------------------
# | [WEB SEARCH] Web Search APIs for LLM Tools                                  |
# ------------------------------------------------------------------------------
#
# Web search providers enable the proxy to perform web searches for LLM tools.
# These APIs are used by the search functionality in the gateway.
# ------------------------------------------------------------------------------

# --- DuckDuckGo Instant Answer API (FREE, NO API KEY REQUIRED) ---
# DuckDuckGo provides free search with no API key or rate limits.
# The gateway uses the Instant Answer API to fetch search results.
# No configuration needed - works out of the box.
#
# Usage: Automatically used when enabled in router_config.yaml

# --- Brave Search API (FREE TIER) ---
# Brave Search API provides high-quality, privacy-focused search results.
# Get your API key at: https://api.search.brave.com/app/keys
# Pricing: 2,000 free requests/month for developers
# API Base: https://api.search.brave.com/res/v1/web/search
BRAVE_API_KEY="YOUR_BRAVE_SEARCH_API_KEY"

# --- Tavily Search API (FREE TIER) ---
# Tavily provides AI-optimized search with context-aware results.
# Get your API key at: https://tavily.com/ (sign up for free account)
# Pricing: 1,000 free searches/month
# API Base: https://api.tavily.com/search
TAVILY_API_KEY="YOUR_TAVILY_API_KEY"

# ------------------------------------------------------------------------------
# | [G4F] g4f Fallback Providers                                               |
# ------------------------------------------------------------------------------
#
# G4F (g4f) is a unified wrapper for multiple free LLM providers.
# Configure these variables to enable G4F fallback routing.
# ------------------------------------------------------------------------------

# G4F API Key (if required by specific providers)
G4F_API_KEY=""

# G4F Provider Base URLs
G4F_MAIN_API_BASE="https://g4f-api.example.com"  # Main g4f-compatible API
G4F_GROQ_API_BASE="https://g4f-groq.example.com"  # Groq-compatible endpoint
G4F_GROK_API_BASE="https://g4f-grok.example.com"  # Grok-compatible endpoint
G4F_GEMINI_API_BASE="https://g4f-gemini.example.com"  # Gemini-compatible endpoint
G4F_NVIDIA_API_BASE="https://g4f-nvidia.example.com"  # NVIDIA-compatible endpoint

# Provider Priority Tiers
# Lower tier number = higher priority (tier 1 is tried first)
# PROVIDER_PRIORITY_G4F=5  # G4F fallback tier (default: lowest priority)
# PROVIDER_PRIORITY_GROQ=2  # Groq direct connection (high priority)
# PROVIDER_PRIORITY_GEMINI=3  # Gemini direct connection


# ==============================================================================
# | [G4F FALLBACK PROVIDERS] Free Tier Fallback Layer                         |
# ==============================================================================
#
# G4F provides free access to multiple LLM providers via https://g4f.dev/
# This integration adds g4f as a fallback layer when primary providers are rate-limited.
#
# To get a free G4F API key, visit: https://g4f.dev/api_key.html
#

# G4F API Key (used for all g4f providers)
# Get free key at: https://g4f.dev/api_key.html
G4F_API_KEY="YOUR_G4F_API_KEY_OR_SECRET"

# G4F Provider Base URLs (these are pre-configured, shown for reference)
# These provide access to different underlying LLM providers through g4f
G4F_MAIN_API_BASE="https://g4f.dev/v1"
G4F_GROQ_API_BASE="https://g4f.dev/api/groq"
G4F_GROK_API_BASE="https://g4f.dev/api/grok"
G4F_GEMINI_API_BASE="https://g4f.dev/api/gemini"
G4F_NVIDIA_API_BASE="https://g4f.dev/api/nvidia"

# ==============================================================================
# | [PROVIDER PRIORITY] Fallback Tier Configuration                           |
# ==============================================================================
#
# Controls which providers are tried first when making requests.
# Lower numbers = higher priority (tried first)
# Tiers:
#   1 = Primary (fastest, most reliable) - Try first
#   2 = Secondary (fallback) - Try if primary is rate-limited
#   3 = Tertiary (last resort) - Try if secondary is rate-limited
#
# Format: PROVIDER_PRIORITY_<PROVIDER_NAME>=<tier_number>
#

# Primary Tier (1) - Your main providers with highest priority
PROVIDER_PRIORITY_ANTIGRAVITY=1
PROVIDER_PRIORITY_GEMINI_CLI=1
PROVIDER_PRIORITY_QWEN_CODE=1
PROVIDER_PRIORITY_IFLOW=1

# Secondary Tier (2) - G4F fallback providers (free tier)
PROVIDER_PRIORITY_G4F_MAIN=2
PROVIDER_PRIORITY_G4F_GROQ=2
PROVIDER_PRIORITY_G4F_GROK=2
PROVIDER_PRIORITY_G4F_GEMINI=2
PROVIDER_PRIORITY_G4F_NVIDIA=2

# Tertiary Tier (3) - Standard API providers (last resort)
PROVIDER_PRIORITY_OPENAI=3
PROVIDER_PRIORITY_ANTHROPIC=3
PROVIDER_PRIORITY_OPENROUTER=3
PROVIDER_PRIORITY_GROQ=3
PROVIDER_PRIORITY_MISTRAL=3
PROVIDER_PRIORITY_NVIDIA=3
PROVIDER_PRIORITY_COHERE=3
PROVIDER_PRIORITY_CHUTES=3
PROVIDER_PRIORITY_BEDROCK=3


# ------------------------------------------------------------------------------
# | [OAUTH] Provider OAuth 2.0 Credentials                                     |
# ------------------------------------------------------------------------------
#
# The proxy now uses a "local-first" approach for OAuth credentials.
# All OAuth credentials are managed within the 'oauth_creds/' directory.
#
# HOW IT WORKS:
# 1. On the first run, if you provide a path to an existing credential file
#    (e.g., from ~/.gemini/), the proxy will COPY it into the local
#    'oauth_creds/' directory with a standardized name (e.g., 'gemini_cli_oauth_1.json').
# 2. On all subsequent runs, the proxy will ONLY use the files found inside
#    'oauth_creds/'. It will no longer scan system-wide directories.
# 3. To add a new account, either use the '--add-credential' tool or manually
#    place a new, valid credential file in the 'oauth_creds/' directory.
#
# Use the variables below for the ONE-TIME setup to import existing credentials.
# After the first successful run, you can clear these paths.
#

# --- Google Gemini (gcloud CLI) ---
# Path to your gcloud ADC file (e.g., ~/.config/gcloud/application_default_credentials.json)
# or a credential file from the official 'gemini' CLI (e.g., ~/.gemini/credentials.json).
GEMINI_CLI_OAUTH_1=""

# --- Qwen / Dashscope (Code Companion) ---
# Path to your Qwen credential file (e.g., ~/.qwen/oauth_creds.json).
QWEN_CODE_OAUTH_1=""

# --- iFlow ---
# Path to your iFlow credential file (e.g., ~/.iflow/oauth_creds.json).
IFLOW_OAUTH_1=""


# ------------------------------------------------------------------------------
# | [ADVANCED] Provider-Specific Settings                                      |
# ------------------------------------------------------------------------------

# --- Gemini CLI Project ID ---
# Required if you are using the Gemini CLI OAuth provider and the proxy
# cannot automatically determine your Google Cloud Project ID.
GEMINI_CLI_PROJECT_ID=""

# --- Model Ignore Lists ---
# Specify a comma-separated list of model names to exclude from a provider's
# available models. This is useful for filtering out models you don't want to use.
#
# Format: IGNORE_MODELS_<PROVIDER_NAME>="model-1,model-2,model-3"
#
# Example:
# IGNORE_MODELS_GEMINI="gemini-1.0-pro-vision-latest,gemini-1.0-pro-latest"
# IGNORE_MODELS_OPENAI="gpt-4-turbo,gpt-3.5-turbo-instruct"
IGNORE_MODELS_GEMINI=""
IGNORE_MODELS_OPENAI=""

# --- Model Whitelists (Overrides Blacklists) ---
# Specify a comma-separated list of model names to ALWAYS include from a
# provider's list. This acts as an override for the ignore list.
#
# HOW IT WORKS:
# 1. A model on a whitelist will ALWAYS be available, even if it's also on an
#    ignore list (or if the ignore list is set to "*").
# 2. For any models NOT on the whitelist, the standard ignore list logic applies.
#
# This allows for two main use cases:
# - "Pure Whitelist" Mode: Set IGNORE_MODELS_<PROVIDER>="*" and then specify
#   only the models you want in WHITELIST_MODELS_<PROVIDER>.
# - "Exemption" Mode: Blacklist a broad range of models (e.g., "*-preview*")
#   and then use the whitelist to exempt specific preview models you want to test.
#
# Format: WHITELIST_MODELS_<PROVIDER_NAME>="model-1,model-2"
#
# Example of a pure whitelist for Gemini:
# IGNORE_MODELS_GEMINI="*"
# WHITELIST_MODELS_GEMINI="gemini-1.5-pro-latest,gemini-1.5-flash-latest"
WHITELIST_MODELS_GEMINI=""
WHITELIST_MODELS_OPENAI=""

# --- Maximum Concurrent Requests Per Key ---
# Controls how many concurrent requests for the SAME model can use the SAME key.
# This is useful for providers that can handle concurrent requests without rate limiting.
# Default is 1 (no concurrency, current behavior).
#
# Format: MAX_CONCURRENT_REQUESTS_PER_KEY_<PROVIDER_NAME>=<number>
#
# Example:
# MAX_CONCURRENT_REQUESTS_PER_KEY_OPENAI=3  # Allow 3 concurrent requests per OpenAI key
# MAX_CONCURRENT_REQUESTS_PER_KEY_GEMINI=1  # Allow only 1 request per Gemini key (default)
#
MAX_CONCURRENT_REQUESTS_PER_KEY_OPENAI=1
MAX_CONCURRENT_REQUESTS_PER_KEY_GEMINI=1
MAX_CONCURRENT_REQUESTS_PER_KEY_ANTHROPIC=1
MAX_CONCURRENT_REQUESTS_PER_KEY_IFLOW=1

# --- Credential Rotation Mode ---
# Controls how credentials are rotated when multiple are available for a provider.
# This affects how the proxy selects the next credential to use for requests.
#
# Available modes:
#   balanced   - (Default) Rotate credentials evenly across requests to distribute load.
#                Best for API keys with per-minute rate limits.
#   sequential - Use one credential until it's exhausted (429 error), then switch to next.
#                Best for credentials with daily/weekly quotas (e.g., free tier accounts).
#                When a credential hits quota, it's put on cooldown based on the reset time
#                parsed from the provider's error response.
#
# Format: ROTATION_MODE_<PROVIDER_NAME>=<mode>
#
# Provider Defaults:
#   - antigravity: sequential (free tier accounts with daily quotas)
#   - All others: balanced
#
# Example:
# ROTATION_MODE_GEMINI=sequential    # Use Gemini keys until quota exhausted
# ROTATION_MODE_OPENAI=balanced      # Distribute load across OpenAI keys (default)
# ROTATION_MODE_ANTIGRAVITY=balanced # Override Antigravity's sequential default
#
# ROTATION_MODE_GEMINI=balanced
# ROTATION_MODE_ANTIGRAVITY=sequential

# --- Priority-Based Concurrency Multipliers ---
# Credentials can be assigned to priority tiers (1=highest, 2, 3, etc.).
# Each tier can have a concurrency multiplier that increases the effective
# concurrent request limit for credentials in that tier.
#
# How it works:
#   effective_concurrent_limit = MAX_CONCURRENT_REQUESTS_PER_KEY * tier_multiplier
#
# This allows paid/premium credentials to handle more concurrent requests than
# free tier credentials, regardless of rotation mode.
#
# Provider Defaults (built into provider classes):
#   Antigravity:
#     Priority 1: 5x (paid ultra tier)
#     Priority 2: 3x (standard paid tier)
#     Priority 3+: 2x (sequential mode) or 1x (balanced mode)
#   Gemini CLI:
#     Priority 1: 5x
#     Priority 2: 3x
#     Others: 1x (all modes)
#
# Format: CONCURRENCY_MULTIPLIER_<PROVIDER>_PRIORITY_<N>=<multiplier>
#
# Mode-specific overrides (optional):
# Format: CONCURRENCY_MULTIPLIER_<PROVIDER>_PRIORITY_<N>_<MODE>=<multiplier>
#
# Examples:
# CONCURRENCY_MULTIPLIER_ANTIGRAVITY_PRIORITY_1=10   # Override P1 to 10x
# CONCURRENCY_MULTIPLIER_ANTIGRAVITY_PRIORITY_3=1    # Override P3 to 1x
# CONCURRENCY_MULTIPLIER_ANTIGRAVITY_PRIORITY_2_BALANCED=1  # P2 = 1x in balanced mode only

# --- Model Quota Groups ---
# Models that share quota/cooldown timing. When one model in a group hits
# quota exhausted (429), all models in the group receive the same cooldown timestamp.
# They also reset (archive stats) together when the quota period expires.
#
# This is useful for providers where multiple model variants share the same
# underlying quota (e.g., Claude Sonnet and Opus on Antigravity).
#
# Format: QUOTA_GROUPS_<PROVIDER>_<GROUP>="model1,model2,model3"
#
# To DISABLE a default group, set it to empty string:
#   QUOTA_GROUPS_ANTIGRAVITY_CLAUDE=""
#
# Default groups:
#   ANTIGRAVITY.CLAUDE: claude-sonnet-4-5,claude-opus-4-5
#
# Examples:
# QUOTA_GROUPS_ANTIGRAVITY_CLAUDE="claude-sonnet-4-5,claude-opus-4-5"
# QUOTA_GROUPS_ANTIGRAVITY_GEMINI="gemini-3-pro-preview,gemini-3-pro-image-preview"

# ------------------------------------------------------------------------------
# | [ADVANCED] Proxy Configuration                                             |
# ------------------------------------------------------------------------------

# --- OAuth Refresh Interval ---
# How often, in seconds, the background refresher should check and refresh
# expired OAuth tokens.
OAUTH_REFRESH_INTERVAL=600 # Default is 600 seconds (10 minutes)

# --- Skip OAuth Initialization ---
# Set to "true" to prevent the proxy from performing the interactive OAuth
# setup/validation flow on startup. This is highly recommended for non-interactive
# environments like Docker containers or automated scripts.
# Ensure your credentials in 'oauth_creds/' are valid before enabling this.
SKIP_OAUTH_INIT_CHECK=false