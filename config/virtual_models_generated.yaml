metadata:
  generated_at: '2026-02-19T18:03:28.201856'
  sources:
  - model_rankings.yaml
  - chat_model_rankings.yaml
  - UGI Leaderboard
virtual_models:
  coding-elite:
    description: Best agentic coding models (SWE-bench + LiveCodeBench weighted)
    fallback_chain:
    - provider: g4f
      model: claude-opus-4-5
      priority: 1
      reasoning_effort: None
    - provider: nvidia
      model: deepseek-v3.2
      priority: 2
      reasoning_effort: High
    - provider: together
      model: deepseek-r1-0528:free
      priority: 3
      reasoning_effort: High
    - provider: together
      model: deepseek-r1:free
      priority: 4
      reasoning_effort: High
    - provider: nvidia
      model: deepseek-r1-0528
      priority: 5
      reasoning_effort: High
    - provider: nvidia
      model: deepseek-r1
      priority: 6
      reasoning_effort: High
    - provider: g4f
      model: gpt-5-2
      priority: 7
      reasoning_effort: Medium
    - provider: gemini
      model: gemini-3-flash
      priority: 8
      reasoning_effort: Low
    - provider: together
      model: deepseek-r1-0528-qwen3-8b:free
      priority: 9
      reasoning_effort: High
    - provider: together
      model: devstral-2512:free
      priority: 10
      reasoning_effort: Medium
    - provider: nvidia
      model: devstral-2-123b-instruct-2512
      priority: 11
      reasoning_effort: Medium
    - provider: g4f
      model: devstral-2512
      priority: 12
      reasoning_effort: Medium
    - provider: g4f
      model: glm-5
      priority: 13
      reasoning_effort: Medium
    - provider: g4f
      model: claude-sonnet-4-5
      priority: 14
      reasoning_effort: Medium
    - provider: gemini
      model: gemini-3-pro
      priority: 15
      reasoning_effort: Low
    - provider: g4f
      model: kimi-k2-5
      priority: 16
      reasoning_effort: None
    - provider: g4f
      model: gpt-5-1
      priority: 17
      reasoning_effort: Medium
    - provider: g4f
      model: claude-haiku-4-5
      priority: 18
      reasoning_effort: None
    - provider: g4f
      model: grok-4
      priority: 19
      reasoning_effort: None
    - provider: together
      model: deepseek-v3
      priority: 20
      reasoning_effort: None
    - provider: together
      model: deepseek-r1t2-chimera:free
      priority: 21
      reasoning_effort: High
    settings:
      timeout_ms: 180000
      retry_on_rate_limit: true
  coding-smart:
    description: High-quality coding with balanced performance
    fallback_chain:
    - provider: g4f
      model: claude-opus-4-5
      priority: 1
      reasoning_effort: None
    - provider: g4f
      model: gpt-5-2
      priority: 2
      reasoning_effort: Medium
    - provider: nvidia
      model: deepseek-v3.2
      priority: 3
      reasoning_effort: High
    - provider: together
      model: deepseek-r1-0528:free
      priority: 4
      reasoning_effort: High
    - provider: together
      model: deepseek-r1:free
      priority: 5
      reasoning_effort: High
    - provider: nvidia
      model: deepseek-r1-0528
      priority: 6
      reasoning_effort: High
    - provider: nvidia
      model: deepseek-r1
      priority: 7
      reasoning_effort: High
    - provider: gemini
      model: gemini-3-flash
      priority: 8
      reasoning_effort: Low
    - provider: together
      model: deepseek-r1-0528-qwen3-8b:free
      priority: 9
      reasoning_effort: High
    - provider: g4f
      model: glm-5
      priority: 10
      reasoning_effort: Medium
    - provider: together
      model: devstral-2512:free
      priority: 11
      reasoning_effort: Medium
    - provider: nvidia
      model: devstral-2-123b-instruct-2512
      priority: 12
      reasoning_effort: Medium
    - provider: g4f
      model: devstral-2512
      priority: 13
      reasoning_effort: Medium
    - provider: g4f
      model: claude-sonnet-4-5
      priority: 14
      reasoning_effort: Medium
    - provider: gemini
      model: gemini-3-pro
      priority: 15
      reasoning_effort: Low
    - provider: g4f
      model: kimi-k2-5
      priority: 16
      reasoning_effort: None
    - provider: g4f
      model: gpt-5-1
      priority: 17
      reasoning_effort: Medium
    - provider: g4f
      model: claude-haiku-4-5
      priority: 18
      reasoning_effort: None
    - provider: g4f
      model: grok-4
      priority: 19
      reasoning_effort: None
    - provider: together
      model: deepseek-v3
      priority: 20
      reasoning_effort: None
    - provider: together
      model: deepseek-r1t2-chimera:free
      priority: 21
      reasoning_effort: High
    - provider: together
      model: deepseek-r1-distill-qwen-14b
      priority: 22
      reasoning_effort: High
    - provider: groq
      model: deepseek-r1-distill-llama-70b
      priority: 23
      reasoning_effort: High
    - provider: groq
      model: deepseek-r1-distill-llama-8b
      priority: 24
      reasoning_effort: High
    - provider: together
      model: deepseek-r1-distill-qwen-1-5b
      priority: 25
      reasoning_effort: High
    - provider: together
      model: qwen-2-5-72b
      priority: 26
      reasoning_effort: None
    - provider: together
      model: codestral-latest
      priority: 27
      reasoning_effort: None
    - provider: gemini
      model: gemini-2-flash
      priority: 28
      reasoning_effort: Low
    settings:
      timeout_ms: 120000
      retry_on_rate_limit: true
  coding-fast:
    description: Fastest coding models (TPS priority with quality floor)
    fallback_chain:
    - provider: cerebras
      model: llama-3-1-8b
      priority: 1
      reasoning_effort: Low
    - provider: g4f
      model: gpt-5-2
      priority: 2
      reasoning_effort: Medium
    settings:
      timeout_ms: 30000
      retry_on_rate_limit: true
  chat-smart:
    description: Best intelligence-to-speed ratio (smart AND reasonably fast)
    fallback_chain: []
    settings:
      timeout_ms: 120000
      retry_on_rate_limit: true
  chat-elite:
    description: Most intelligent models regardless of speed (pure intelligence ranking)
    fallback_chain:
    - provider: gemini
      model: gemini-3-pro
      priority: 1
      reasoning_effort: None
    - provider: g4f
      model: claude-opus-4-5-thinking-32k
      priority: 2
      reasoning_effort: None
    - provider: g4f
      model: gpt-5-xhigh
      priority: 3
      reasoning_effort: None
    - provider: g4f
      model: grok-4-1-thinking
      priority: 4
      reasoning_effort: None
    - provider: gemini
      model: gemini-3-flash
      priority: 5
      reasoning_effort: None
    - provider: g4f
      model: claude-sonnet-4-5-thinking-32k
      priority: 6
      reasoning_effort: None
    - provider: g4f
      model: gpt-5-high
      priority: 7
      reasoning_effort: None
    - provider: g4f
      model: grok-4-1
      priority: 8
      reasoning_effort: None
    - provider: groq
      model: llama-4-scout
      priority: 9
      reasoning_effort: None
    - provider: together
      model: medium-3-1
      priority: 10
      reasoning_effort: None
    - provider: gemini
      model: gemini-2-flash
      priority: 11
      reasoning_effort: None
    - provider: g4f
      model: gpt-5-medium
      priority: 12
      reasoning_effort: None
    - provider: together
      model: deepseek-v3
      priority: 13
      reasoning_effort: None
    - provider: together
      model: qwen2-5-72b
      priority: 14
      reasoning_effort: None
    settings:
      timeout_ms: 300000
      retry_on_rate_limit: true
  chat-fast:
    description: Fastest models that aren't stupid (TPS priority, min intelligence
      threshold)
    fallback_chain:
    - provider: together
      model: medium-3-1
      priority: 1
      reasoning_effort: None
    - provider: gemini
      model: gemini-2-flash
      priority: 2
      reasoning_effort: None
    - provider: groq
      model: llama-4-scout
      priority: 3
      reasoning_effort: None
    - provider: g4f
      model: gpt-5-medium
      priority: 4
      reasoning_effort: None
    settings:
      timeout_ms: 15000
      retry_on_rate_limit: true
  chat-rp:
    description: Uncensored RP models (min UGI threshold, then sorted by TPS for fast
      responses)
    fallback_chain:
    - provider: g4f
      model: grok-4-0709
      priority: 1
      reasoning_effort: None
    - provider: g4f
      model: claude-opus-4-6 (adaptive, effort=high)
      priority: 2
      reasoning_effort: None
    - provider: g4f
      model: claude-opus-4-6 (adaptive, effort=max)
      priority: 3
      reasoning_effort: None
    - provider: g4f
      model: claude-opus-4-6 (adaptive, effort=medium)
      priority: 4
      reasoning_effort: None
    - provider: g4f
      model: grok-3
      priority: 5
      reasoning_effort: None
    - provider: g4f
      model: deepseek-v3.2-speciale
      priority: 6
      reasoning_effort: None
    - provider: g4f
      model: o3-2025-04-16 (reasoning_effort=low)
      priority: 7
      reasoning_effort: None
    - provider: g4f
      model: kimi-k2.5 (reasoning=enabled)
      priority: 8
      reasoning_effort: None
    - provider: g4f
      model: claude-opus-4-6 (adaptive, effort=low)
      priority: 9
      reasoning_effort: None
    - provider: g4f
      model: claude-sonnet-4-6 (adaptive, effort=max)
      priority: 10
      reasoning_effort: None
    - provider: g4f
      model: gemini-3-flash-preview (thinking_level=medium)
      priority: 11
      reasoning_effort: None
    - provider: g4f
      model: claude-sonnet-4-6 (adaptive, effort=medium)
      priority: 12
      reasoning_effort: None
    - provider: g4f
      model: gpt-4.1-2025-04-14
      priority: 13
      reasoning_effort: None
    - provider: g4f
      model: o3-2025-04-16 (reasoning_effort=medium)
      priority: 14
      reasoning_effort: None
    - provider: g4f
      model: gemini-3-flash-preview (thinking_level=minimal)
      priority: 15
      reasoning_effort: None
    - provider: g4f
      model: deepseek-v3.2 (reasoning=enabled)
      priority: 16
      reasoning_effort: None
    - provider: g4f
      model: o3-2025-04-16 (reasoning_effort=high)
      priority: 17
      reasoning_effort: None
    - provider: g4f
      model: grok-4-1-fast-reasoning
      priority: 18
      reasoning_effort: None
    - provider: g4f
      model: gemini-3-flash-preview (thinking_level=high)
      priority: 19
      reasoning_effort: None
    - provider: g4f
      model: claude-opus-4-5-20251101 (thinking=disabled)
      priority: 20
      reasoning_effort: None
    settings:
      timeout_ms: 30000
      retry_on_rate_limit: true
providers:
  nvidia:
    base_url: https://integrate.api.nvidia.com/v1
    api_key_env: NVIDIA_API_KEY
    rate_limits:
      rpm: 60
  together:
    base_url: https://api.together.xyz/v1
    api_key_env: TOGETHER_API_KEY
    rate_limits:
      rpm: 60
  cerebras:
    base_url: https://api.cerebras.ai/v1
    api_key_env: CEREBRAS_API_KEY
    rate_limits:
      rpm: 50
      daily: 5000
  groq:
    base_url: https://api.groq.com/openai/v1
    api_key_env: GROQ_API_KEY
    rate_limits:
      rpm: 30
      daily: 14400
  google:
    base_url: https://generativelanguage.googleapis.com/v1beta
    api_key_env: GEMINI_API_KEY
    rate_limits:
      rpm: 15
      daily: 1500
  gemini:
    base_url: https://generativelanguage.googleapis.com/v1beta
    api_key_env: GEMINI_API_KEY
    rate_limits:
      rpm: 15
      daily: 1500
  openai:
    base_url: https://api.openai.com/v1
    api_key_env: OPENAI_API_KEY
    rate_limits:
      rpm: 3000
  mistral:
    base_url: https://api.mistral.ai/v1
    api_key_env: MISTRAL_API_KEY
    rate_limits:
      rpm: 10
  g4f:
    base_url: https://g4f.dev/v1
    api_key_env: G4F_API_KEY
    rate_limits:
      rpm: 10
      daily: 100
  kilo:
    base_url: https://gateway.kilo.ai/v1
    api_key_env: KILO_API_KEY
    rate_limits:
      rpm: 200
      hourly: 200
    free_tier: true
    description: Free GLM-5 access (z-ai/glm-5:free)
