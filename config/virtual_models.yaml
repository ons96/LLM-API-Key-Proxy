virtual_models:
  # =============================================================================
  # CODING ELITE (Best Agentic Coding Models by Quality Score)
  # =============================================================================
  # Ordered by SWE-bench / agentic coding score (best to worst)
  # Then by provider speed within each tier
  coding-elite:
    description: "Best agentic coding models - ordered by SWE-bench score (quality first), then provider speed"
    fallback_chain:
      # Tier 1: TOP TIER (>74.0 SWE-bench score)
      # 1. Claude Opus 4.5 (74.4) - Best agentic coding, use multiple providers
      - provider: g4f
        model: "claude-opus-4-5"
        priority: 1

      - provider: openrouter
        model: "anthropic/claude-opus-4-5"
        priority: 1

      # 2. Gemini 3 Pro (74.2)
      - provider: google
        model: "gemini-3-pro"
        priority: 2
      - provider: zenmux
        model: "gemini-3-pro"
        priority: 2
      - provider: iflow
        model: "gemini-3-pro"
        priority: 2

      # Tier 2: HIGH TIER (70.0-73.9 SWE-bench score)
      # 3. GPT-5.2 (71.8)
      - provider: openai
        model: "gpt-5-2"
        priority: 3

      # 4. Claude Sonnet 4.5 (70.6)
      - provider: g4f
        model: "claude-sonnet-4-5"
        priority: 4

      - provider: openrouter
        model: "anthropic/claude-sonnet-4-5"
        priority: 4

      # Tier 3: SOLID TIER (Your actual working providers with good quality)
      # 5. GPT-4o (working on OpenAI)
      - provider: openai
        model: "gpt-4o"
        priority: 5

      # 6. GPT-4 (G4F fallback)
      - provider: g4f
        model: "gpt-4"
        priority: 6

      # 7. Gemini 3 Pro (has excellent coding ability)
      - provider: gemini
        model: "gemini-3-pro"
        priority: 7

      # Tier 4: FAST & RELIABLE (Your working free providers with decent quality)
      # 8. Llama 3.3 70B (Groq - fastest free option)
      - provider: groq
        model: "llama-3.3-70b-versatile"
        priority: 8

      # 9. Llama 3.3 70B (Cerebras - ultra fast)
      - provider: cerebras
        model: "llama-3.3-70b"
        priority: 9

      # 10. Mistral Codestral
      - provider: mistral
        model: "codestral-latest"
        priority: 10

    settings:
      timeout_ms: 180000
      retry_on_rate_limit: true

  # =============================================================================
  # CODING FAST (ULTRA FAST - FOR QUICK EDITS)
  # =============================================================================
  coding-fast:
    description: "Highest TPS coding models - Cerebras fastest, then Groq, then others"
    fallback_chain:
      # 1. Cerebras: Llama 3.1 70B (~3000 t/s) - FASTEST
      - provider: cerebras
        model: "llama-3.1-70b"
        priority: 1

      # 2. Groq: Llama 3.3 70B (~1000 t/s)
      - provider: groq
        model: "llama-3.3-70b-versatile"
        priority: 2

      # 3. Cerebras: Llama 3.1 8B (~3000 t/s)
      - provider: cerebras
        model: "llama-3.1-8b"
        priority: 3

      # 4. Groq: Llama 3.1 8B (~1000 t/s)
      - provider: groq
        model: "llama-3.1-8b-instant"
        priority: 4

      # 5. OpenAI: gpt-4o-mini
      - provider: openai
        model: "gpt-4o-mini"
        priority: 5

      # 6. Gemini: Flash
      - provider: gemini
        model: "gemini-2-5-flash"
        priority: 6

      # Fallback
      - provider: openrouter
        model: "google/gemini-flash-1.5"
        priority: 7

      - provider: g4f
        model: "gpt-4"
        priority: 8

    settings:
      timeout_ms: 30000
      retry_on_rate_limit: true

  # =============================================================================
  # CHAT SMART (REASONING - BEST MODELS FROM LEADERBOARDS)
  # =============================================================================
  chat-smart:
    description: "Best chat/reasoning models - ranked by Chatbot Arena and AI leaderboards"
    fallback_chain:
      # Tier 1: TOP RANKED CHAT/REASONING MODELS (2025 benchmarks)
      # 1. Claude Opus 4.5 (Anthropic - #2 intelligence per Artificial Analysis)
      - provider: anthropic
        model: "claude-opus-4-5"
        priority: 1

      - provider: g4f
        model: "claude-opus-4-5"
        priority: 1

      # 2. GPT-5.2 (OpenAI - highest reasoning performance)
      - provider: openai
        model: "gpt-5-2"
        priority: 2

      # 3. Claude Opus 4.5 Thinking-32k (extended context)
      - provider: g4f
        model: "claude-opus-4.5-thinking-32k"
        priority: 3

      # 4. GPT-5.1 (OpenAI - new flagship)
      - provider: openai
        model: "gpt-5.1"
        priority: 4

      # 5. Claude Sonnet 4.5 (balance quality/speed)
      - provider: anthropic
        model: "claude-sonnet-4-5"
        priority: 5

      - provider: g4f
        model: "claude-sonnet-4-5"
        priority: 5

      # 6. GPT-4o (strong general purpose)
      - provider: openai
        model: "gpt-4o"
        priority: 6

      - provider: g4f
        model: "gpt-4o"
        priority: 6

      # Tier 2: HIGH PERFORMING FREE FAST OPTIONS
      # 7. Groq Llama 3.3 70B (fast free option, good quality)
      - provider: groq
        model: "llama-3.3-70b-versatile"
        priority: 7

      # 8. Cerebras Llama 3.3 70B (ultra fast)
      - provider: cerebras
        model: "llama-3.3-70b"
        priority: 8

      # 9. Claude Sonnet 4.5 Thinking (extended context)
      - provider: g4f
        model: "claude-sonnet-4.5-thinking-32k"
        priority: 9

      # 10. Gemini 3 Pro (still good, but superseded)
      - provider: gemini
        model: "gemini-3-pro"
        priority: 10

      # 11. Gemini 2.5 Flash (fast chat, good reasoning)
      - provider: gemini
        model: "gemini-2-5-flash"
        priority: 11

      # Fallback
      - provider: openai
        model: "gpt-4o-mini"
        priority: 12

      - provider: g4f
        model: "gpt-3.5-turbo"
        priority: 13

    settings:
      timeout_ms: 180000

  # =============================================================================
  # CHAT FAST (LOWEST LATENCY)
  # =============================================================================
  chat-fast:
    description: "Fastest chat models (Cerebras/Groq/Gemini Flash/OpenAI)"
    fallback_chain:
      # 1. Cerebras: Llama 3.1 8B (~3000 t/s)
      - provider: cerebras
        model: "llama-3.1-8b"
        priority: 1

      # 2. Groq: Llama 3.1 8B (~1000 t/s)
      - provider: groq
        model: "llama-3.1-8b-instant"
        priority: 2

      # 3. Gemini Flash
      - provider: gemini
        model: "gemini-2-5-flash"
        priority: 3

      # 4. OpenAI gpt-3.5-turbo
      - provider: openai
        model: "gpt-3.5-turbo"
        priority: 4

      # 5. Mistral Nemo
      - provider: mistral
        model: "mistral-nemo"
        priority: 5

      # Fallback
      - provider: g4f
        model: "gpt-3.5-turbo"
        priority: 6

    settings:
      timeout_ms: 15000

  # =============================================================================
  # CHAT RP (Roleplay - UGI Leaderboard RP Models - Fast + Many Fallbacks)
  # =============================================================================
  chat-rp:
    description: "Best NSFW RP models from UGI leaderboard - prioritizes response time with maximum fallbacks"
    fallback_chain:
      # UGI TOP RANKED RP MODELS (via G4F providers)
      # 1. MN-Violet-Lotus-12B (UGI top RP model, highly rated)
      - provider: g4f
        model: "mn-violet-lotus-12b"
        priority: 1

      # 2. Violet_Twilight-v0.2 (Second best)
      - provider: g4f
        model: "violet-twilight-v0.2"
        priority: 2

      # 3. Captain-Eris_Violet-V0.420
      - provider: g4f
        model: "captain-eris-violet-v0.420"
        priority: 3

      # 4. MN-12B-Lyra-v4
      - provider: g4f
        model: "mn-12b-lyra-v4"
        priority: 4

      # 5. MN-Chinofun-12B-4.1
      - provider: g4f
        model: "mn-chinofun-12b"
        priority: 5

      # 6. L3.1-RP-Hero-InBetween-8B
      - provider: g4f
        model: "l3.1-rp-hero-inbetween-8b"
        priority: 6

      # FASTER RP OPTIONS (for quick character responses)
      # 7. Cerebras 8B (fastest)
      - provider: cerebras
        model: "llama-3.1-8b"
        priority: 7

      # 8. Groq 8B
      - provider: groq
        model: "llama-3.1-8b-instant"
        priority: 8

      # 9. GPT-4o-mini (fast general model for RP)
      - provider: openai
        model: "gpt-4o-mini"
        priority: 9

      # 10. L3.1-RP-Hero-Dirty_Harry-8B
      - provider: g4f
        model: "l3.1-rp-hero-dirty-harry-8b"
        priority: 10

      # MORE FALLBACKS
      - provider: mistral
        model: "mistral-nemo"
        priority: 11

      - provider: groq
        model: "llama-3.3-70b-versatile"
        priority: 12

      - provider: g4f
        model: "gpt-4o-mini"
        priority: 13

    settings:
      timeout_ms: 15000

# =============================================================================
# PROVIDERS (New Free Providers Added)
# =============================================================================
providers:
  # --- New Aggregators ---
  zenmux:
    base_url: "https://api.zenmux.com/v1"
    api_key_env: "ZENMUX_API_KEY"
    rate_limits: { rpm: 20, daily: 500 }

  iflow:
    base_url: "https://apis.iflow.cn/v1"
    api_key_env: "IFLOW_API_KEY"
    rate_limits: { rpm: 60 }

  opencode_zen:
    base_url: "https://api.opencodezen.com/v1"
    api_key_env: "OPENCODE_ZEN_API_KEY"
    rate_limits: { rpm: 50 }

  aihubmix:
    base_url: "https://api.aihubmix.com/v1"
    api_key_env: "AIHUBMIX_API_KEY"
    rate_limits: { rpm: 50 }

  routeway:
    base_url: "https://routeway.ai/v1"
    api_key_env: "ROUTEWAY_API_KEY"
    rate_limits: { rpm: 20, daily: 200 }

  modelscope:
    base_url: "https://api-inference.modelscope.cn/v1"
    api_key_env: "MODELSCOPE_API_KEY"
    rate_limits: { rpm: 60 }

  # --- New Direct Providers ---
  nvidia:
    base_url: "https://integrate.api.nvidia.com/v1"
    api_key_env: "NVIDIA_API_KEY"
    rate_limits: { rpm: 60 }

  sambanova:
    base_url: "https://api.sambanova.ai/v1"
    api_key_env: "SAMBANOVA_API_KEY"
    rate_limits: { rpm: 60 }

  mistral:
    base_url: "https://api.mistral.ai/v1"
    api_key_env: "MISTRAL_API_KEY"
    rate_limits: { rpm: 10 }

  # --- Existing ---
  google:
    base_url: "https://generativelanguage.googleapis.com/v1beta"
    api_key_env: "GEMINI_API_KEY"
    rate_limits: { rpm: 15, daily: 1500 }

  groq:
    base_url: "https://api.groq.com/openai/v1"
    api_key_env: "GROQ_API_KEY"
    rate_limits: { rpm: 30, daily: 14400 }

  cerebras:
    base_url: "https://api.cerebras.ai/v1"
    api_key_env: "CEREBRAS_API_KEY"
    rate_limits: { rpm: 50, daily: 5000 }
  
  g4f:
    base_url: "https://g4f.dev/v1"
    api_key_env: "G4F_API_KEY"
    rate_limits: { rpm: 10, daily: 100 }

  puter:
    base_url: "https://puter.com/api/v1" # Needs adapter logic
    api_key_env: "PUTER_API_KEY"
    rate_limits: { rpm: 20 }
