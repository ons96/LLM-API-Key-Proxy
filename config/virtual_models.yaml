virtual_models:
  # =============================================================================
  # CODING ELITE (Best Agentic Coding Models by Quality Score)
  # =============================================================================
  coding-elite:
    description: "Best agentic coding models - including Llama 4, Gemini 3, and Qwen 3"
    fallback_chain:
      # Tier 1: TOP TIER
      # Llama 4 Maverick (Meta)
      - provider: groq
        model: "meta-llama/llama-4-maverick-17b-128e-instruct"
        priority: 1
      - provider: nvidia
        model: "meta/llama-4-maverick-17b-128e-instruct"
        priority: 1

      # Gemini 3 Pro (Google)
      - provider: gemini
        model: "gemini-3-pro-preview"
        priority: 2
      - provider: google
        model: "gemini-3-pro-preview"
        priority: 2

      # Qwen 3 Coder 480B (Open Weights)
      - provider: nvidia
        model: "qwen/qwen3-coder-480b-a35b-instruct"
        priority: 3
      - provider: together
        model: "Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8"
        priority: 3

      # Tier 2: HIGH TIER
      # Qwen 3 235B (Cerebras)
      - provider: cerebras
        model: "qwen-3-235b-a22b-instruct-2507"
        priority: 4

      # Claude Opus 4.5
      - provider: g4f
        model: "claude-opus-4-5"
        priority: 5

      # Tier 3: RELIABLE FALLBACKS
      # GPT-5.2
      - provider: openai
        model: "gpt-5-2"
        priority: 6

      # Gemini 2.5 Pro
      - provider: gemini
        model: "gemini-2.5-pro"
        priority: 7

    settings:
      timeout_ms: 180000
      retry_on_rate_limit: true

  # =============================================================================
  # CODING FAST (ULTRA FAST - FOR QUICK EDITS)
  # =============================================================================
  coding-fast:
    description: "Highest TPS coding models - Gemini 2.5, Llama 3.1, and Qwen 3"
    fallback_chain:
      # Cerebras: Llama 3.1 70B
      - provider: cerebras
        model: "llama-3.3-70b"
        priority: 1

      # Groq: Llama 3.3 70B
      - provider: groq
        model: "llama-3.3-70b-versatile"
        priority: 2

      # Gemini: 2.5 Flash
      - provider: gemini
        model: "gemini-2.5-flash"
        priority: 3

      # Together: Llama 3.1 8B (High Speed)
      - provider: together
        model: "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
        priority: 4

      # Google: Gemma 3 27B
      - provider: gemini
        model: "gemma-3-27b-it"
        priority: 5

      # Cerebras: Qwen 3 32B
      - provider: cerebras
        model: "qwen-3-32b"
        priority: 6

    settings:
      timeout_ms: 30000
      retry_on_rate_limit: true

  # =============================================================================
  # CHAT SMART (REASONING - BEST MODELS FROM LEADERBOARDS)
  # =============================================================================
  chat-smart:
    description: "Best chat/reasoning models - ranked by Chatbot Arena and AI leaderboards"
    fallback_chain:
      # Tier 1: Flagship Models
      # DeepSeek V3.2
      - provider: nvidia
        model: "deepseek-ai/deepseek-v3.2"
        priority: 1
      - provider: together
        model: "deepseek-ai/DeepSeek-V3"
        priority: 1

      # Gemini 3 Pro
      - provider: gemini
        model: "gemini-3-pro-preview"
        priority: 2
      - provider: google
        model: "gemini-3-pro-preview"
        priority: 2

      # GPT-5.1
      - provider: openai
        model: "gpt-5.1"
        priority: 3

      # Tier 2: Free High Performance
      # Together MoA-1 Turbo (Mixture of Agents)
      - provider: together
        model: "togethercomputer/MoA-1-Turbo"
        priority: 4

      # Llama 3.3 70B (Groq)
      - provider: groq
        model: "llama-3.3-70b-versatile"
        priority: 5

      # Gemini 2.5 Pro
      - provider: gemini
        model: "gemini-2.5-pro"
        priority: 6

    settings:
      timeout_ms: 180000
      retry_on_rate_limit: true

  # =============================================================================
  # CHAT FAST (LOWEST LATENCY)
  # =============================================================================
  chat-fast:
    description: "Fastest chat models (Lowest Latency)"
    fallback_chain:
      # Gemini 2.0 Flash Lite
      - provider: gemini
        model: "gemini-2.0-flash-lite"
        priority: 1

      # Llama 3.1 8B Turbo
      - provider: together
        model: "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
        priority: 2

      # Groq Llama 3.1 8B
      - provider: groq
        model: "llama-3.1-8b-instant"
        priority: 3

      # Gemini 2.5 Flash
      - provider: gemini
        model: "gemini-2.5-flash"
        priority: 4

      # Gemma 3 4B
      - provider: gemini
        model: "gemma-3-4b-it"
        priority: 5

    settings:
      timeout_ms: 15000
      retry_on_rate_limit: true

  # =============================================================================
  # CHAT RP (Roleplay - UGI Leaderboard RP Models)
  # =============================================================================
  chat-rp:
    description: "Best RP models from UGI leaderboard"
    fallback_chain:
      # UGI TOP RANKED RP MODELS
      - provider: g4f
        model: "mn-violet-lotus-12b"
        priority: 1
      - provider: g4f
        model: "violet-twilight-v0.2"
        priority: 2
      - provider: g4f
        model: "captain-eris-violet-v0.420"
        priority: 3
      - provider: g4f
        model: "mn-12b-lyra-v4"
        priority: 4

      # FASTER RP OPTIONS
      - provider: cerebras
        model: "llama-3.1-8b"
        priority: 5
      - provider: groq
        model: "llama-3.1-8b-instant"
        priority: 6

    settings:
      timeout_ms: 30000
      retry_on_rate_limit: true

# =============================================================================
# PROVIDERS (Registered APIs)
# =============================================================================
providers:
  nvidia:
    base_url: "https://integrate.api.nvidia.com/v1"
    api_key_env: "NVIDIA_API_KEY"
    rate_limits: { rpm: 60 }

  together:
    base_url: "https://api.together.xyz/v1"
    api_key_env: "TOGETHER_API_KEY"
    rate_limits: { rpm: 60 }

  cerebras:
    base_url: "https://api.cerebras.ai/v1"
    api_key_env: "CEREBRAS_API_KEY"
    rate_limits: { rpm: 50, daily: 5000 }

  groq:
    base_url: "https://api.groq.com/openai/v1"
    api_key_env: "GROQ_API_KEY"
    rate_limits: { rpm: 30, daily: 14400 }

  google:
    base_url: "https://generativelanguage.googleapis.com/v1beta"
    api_key_env: "GEMINI_API_KEY"
    rate_limits: { rpm: 15, daily: 1500 }

  gemini:
    base_url: "https://generativelanguage.googleapis.com/v1beta"
    api_key_env: "GEMINI_API_KEY"
    rate_limits: { rpm: 15, daily: 1500 }

  openai:
    base_url: "https://api.openai.com/v1"
    api_key_env: "OPENAI_API_KEY"
    rate_limits: { rpm: 3000 }

  mistral:
    base_url: "https://api.mistral.ai/v1"
    api_key_env: "MISTRAL_API_KEY"
    rate_limits: { rpm: 10 }

  g4f:
    base_url: "https://g4f.dev/v1"
    api_key_env: "G4F_API_KEY"
    rate_limits: { rpm: 10, daily: 100 }

