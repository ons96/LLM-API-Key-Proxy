virtual_models:
  # =============================================================================
  # ELITE CODING MODELS (Highest reasoning/coding capability)
  # =============================================================================
  # Uses top-ranked models from LiveBench Agentic Coding + LMArena WebDev
  # Priority 1-3: Best coding models available
  # Priority 4-5: Excellent fallbacks with different strengths
  # Priority 6: Free-tier accessible model
  coding-elite:
    description: "Top-tier models for complex architecture, debugging, and refactoring"
    fallback_chain:
      # Claude 4.5 Opus Thinking - #1 in LiveBench Agentic Coding (75.61%)
      - provider: openrouter
        model: "anthropic/claude-opus-4-2025-02-19"
        priority: 1
        conditions:
          max_daily_requests: 5
      # GPT-5.2 High - Excellent coding performance, available on Azure/OpenAI
      - provider: openai
        model: "gpt-5.2"
        priority: 2
        conditions:
          max_daily_requests: 10
      # Gemini 3 Pro Preview - Strong reasoning, large context (1M)
      - provider: google
        model: "gemini-3-pro"
        priority: 3
        conditions:
          max_daily_requests: 50
      # DeepSeek V3.2 Thinking - Great reasoning, open weights
      - provider: deepseek
        model: "deepseek-chat"
        priority: 4
      # GLM-4.7 - Strong open model (Z.ai)
      - provider: openrouter
        model: "zhipuai/glm-4-7b-chat"
        priority: 5
      # Qwen3 235B - Large open model, great for reasoning
      - provider: fireworks
        model: "qwen3-235b-a22b"
        priority: 6
    settings:
      timeout_ms: 180000
      retry_on_rate_limit: true
      auto_order: true

  # =============================================================================
  # FAST CODING MODELS (Speed prioritized)
  # =============================================================================
  # Uses Groq/Cerebras for extreme speed, falls back to Flash models
  coding-fast:
    description: "Fast models for simple scripts, autocomplete, quick fixes"
    fallback_chain:
      # Groq: Llama 3.1 70B - Extremely fast (~1000 t/s)
      - provider: groq
        model: "llama-3.1-70b-versatile"
        priority: 1
      # Groq: Llama 3.2 90B - Newer, faster
      - provider: groq
        model: "llama-3.2-90b-vision-preview"
        priority: 2
      # Gemini 3 Flash - Fast + good reasoning
      - provider: google
        model: "gemini-3-flash"
        priority: 3
      # Kimi K2 Thinking Turbo - Very fast on Fireworks
      - provider: fireworks
        model: "kimi-k2-thinking-turbo"
        priority: 4
      # GPT-5 Mini High - Fast from OpenAI
      - provider: openai
        model: "gpt-5-mini"
        priority: 5
    settings:
      timeout_ms: 30000
      retry_on_rate_limit: true

  # =============================================================================
  # WEBDEV SPECIALIST (LMArena WebDev leaderboard optimized)
  # =============================================================================
  webdev-elite:
    description: "Best models specifically for web development tasks"
    fallback_chain:
      # Claude Sonnet 4.5 Thinking - #1 in LMArena WebDev for Sonnet
      - provider: openrouter
        model: "anthropic/claude-sonnet-4-2025-02-19"
        priority: 1
      # Claude Opus 4.5 - Best overall
      - provider: openrouter
        model: "anthropic/claude-opus-4-2025-02-19"
        priority: 2
      # GPT-5.1 Medium - Great for webdev
      - provider: openai
        model: "gpt-5.1-medium"
        priority: 3
      # Gemini 3 Flash Thinking Minimal - Fast + capable
      - provider: google
        model: "gemini-3-flash-thinking"
        priority: 4
      # DeepSeek V3.2 - Great for code
      - provider: deepseek
        model: "deepseek-chat"
        priority: 5
    settings:
      timeout_ms: 120000
      retry_on_rate_limit: true

  # =============================================================================
  # REASONING/REASONING (Complex problem solving)
  # =============================================================================
  reasoning-elite:
    description: "Models with excellent reasoning capabilities (Math, Science, Logic)"
    fallback_chain:
      # Claude Opus 4.5 Thinking - Best reasoning scores
      - provider: openrouter
        model: "anthropic/claude-opus-4-2025-02-19"
        priority: 1
      # GPT-5.2 High - Strong reasoning
      - provider: openai
        model: "gpt-5.2"
        priority: 2
      # Gemini 3 Pro - Large context for complex reasoning
      - provider: google
        model: "gemini-3-pro"
        priority: 3
      # DeepSeek V3.2 Thinking - Great for logic
      - provider: deepseek
        model: "deepseek-reasoner"
        priority: 4
      # Qwen3 235B Thinking - Open model with reasoning focus
      - provider: fireworks
        model: "qwen3-235b-a22b-thinking"
        priority: 5
    settings:
      timeout_ms: 180000
      retry_on_rate_limit: true

  # =============================================================================
  # FAST CHAT (Speed prioritized for simple conversation)
  # =============================================================================
  chat-fast:
    description: "Extremely fast models for simple chat and quick queries"
    fallback_chain:
      # Groq: Llama 3.1 8B - Fastest option (~1000+ t/s)
      - provider: groq
        model: "llama-3.1-8b-instant"
        priority: 1
      # Groq: Llama 3.2 1B - Tiny and instant
      - provider: groq
        model: "llama-3.2-1b-preview"
        priority: 2
      # Fireworks: Qwen 3 30B - Fast + capable
      - provider: fireworks
        model: "qwen3-30b-a3b"
        priority: 3
      # Gemini 3 Flash - Google's fastest
      - provider: google
        model: "gemini-3-flash"
        priority: 4
      # GPT-5 Mini - Fast from OpenAI
      - provider: openai
        model: "gpt-5-mini"
        priority: 5
    settings:
      timeout_ms: 15000

  # =============================================================================
  # GENERAL PURPOSE (Balanced quality and speed)
  # =============================================================================
  chat-balanced:
    description: "Balanced models for general conversation"
    fallback_chain:
      # Claude Sonnet 4.5 - Great balance
      - provider: openrouter
        model: "anthropic/claude-sonnet-4-2025-02-19"
        priority: 1
      # GPT-5.1 Medium - Excellent balance
      - provider: openai
        model: "gpt-5.1-medium"
        priority: 2
      # Gemini 3 Pro - Large context
      - provider: google
        model: "gemini-3-pro"
        priority: 3
      # DeepSeek V3.2 - Great value
      - provider: deepseek
        model: "deepseek-chat"
        priority: 4
      # GLM-4.7 - Open model alternative
      - provider: fireworks
        model: "glm-4-7b-chat"
        priority: 5
    settings:
      timeout_ms: 60000

# =============================================================================
# PROVIDER CONFIGURATIONS
# =============================================================================
providers:
  # OpenRouter - Aggregator with many models
  openrouter:
    base_url: "https://openrouter.ai/api/v1"
    api_key_env: "OPENROUTER_API_KEY"
    rate_limits:
      rpm: 50
      daily: 1000

  # OpenAI - Direct access
  openai:
    base_url: "https://api.openai.com/v1"
    api_key_env: "OPENAI_API_KEY"
    rate_limits:
      rpm: 500
      daily: 10000

  # Google Gemini
  google:
    base_url: "https://generativelanguage.googleapis.com/v1beta"
    api_key_env: "GEMINI_API_KEY"
    rate_limits:
      rpm: 60
      daily: 1500

  # DeepSeek
  deepseek:
    base_url: "https://api.deepseek.com/v1"
    api_key_env: "DEEPSEEK_API_KEY"
    rate_limits:
      rpm: 100
      daily: 10000

  # Groq - Extremely fast inference
  groq:
    base_url: "https://api.groq.com/openai/v1"
    api_key_env: "GROQ_API_KEY"
    rate_limits:
      rpm: 30
      daily: 14400

  # Fireworks AI - Fast open models
  fireworks:
    base_url: "https://api.fireworks.ai/inference/v1"
    api_key_env: "FIREWORKS_API_KEY"
    rate_limits:
      rpm: 100
      daily: 10000
