virtual_models:
  # =============================================================================
  # CODING ELITE (Best Agentic Coding Models - Ranked by SWE-bench)
  # =============================================================================
  coding-elite:
    description: "Best agentic coding models - prioritizes highest SWE-bench scores (o4-mini, Gemini 2.5 Pro, GPT-5.1)"
    fallback_chain:
      # Tier 1: Highest Verified Agentic Performance
      # OpenAI o4-mini (High Effort - 69.8 SWE-bench)
      - provider: openai
        model: "o4-mini"
        priority: 1

      # Gemini 2.5 Pro (68.9 SWE-bench)
      - provider: gemini
        model: "gemini-2.5-pro"
        priority: 2
      - provider: google
        model: "gemini-2.5-pro"
        priority: 2

      # GPT-5.1 Codex Max (68.0 SWE-bench)
      - provider: openai
        model: "gpt-5.1"
        priority: 3

      # Claude Sonnet 4.5 Thinking (64.5 SWE-bench)
      - provider: g4f
        model: "claude-sonnet-4-5-thinking"
        priority: 4

      # Gemini 2.5 Flash Reasoning (65.0 SWE-bench)
      - provider: gemini
        model: "gemini-2.5-flash-reasoning"
        priority: 5

      # Claude Opus 4.5 (62.0 SWE-bench)
      - provider: g4f
        model: "claude-opus-4-5"
        priority: 6

    settings:
      timeout_ms: 180000
      retry_on_rate_limit: true

  # =============================================================================
  # CODING FAST (ULTRA FAST - FOR QUICK EDITS)
  # =============================================================================
  coding-fast:
    description: "Highest TPS coding models - Ranked by Tokens per Second (Cerebras, Groq, SambaNova)"
    fallback_chain:
      # Cerebras Llama 3.3 70B (2,483 TPS)
      - provider: cerebras
        model: "llama-3.3-70b"
        priority: 1

      # SambaNova Llama 3.2 3B (1,601 TPS)
      - provider: sambanova
        model: "llama-3.2-3b"
        priority: 2

      # Groq Llama 3 8B (1,349 TPS)
      - provider: groq
        model: "llama-3-8b-instant"
        priority: 3

      # SambaNova Llama 3.1 8B (1,178 TPS)
      - provider: sambanova
        model: "llama-3.1-8b"
        priority: 4

      # Groq Llama 3.1 8B (835 TPS)
      - provider: groq
        model: "llama-3.1-8b-instant"
        priority: 5

      # Gemini 2.0 Flash Lite (Ultra Low Latency)
      - provider: gemini
        model: "gemini-2.0-flash-lite"
        priority: 6

    settings:
      timeout_ms: 30000
      retry_on_rate_limit: true

  # =============================================================================
  # CHAT SMART (REASONING - BEST MODELS FROM LEADERBOARDS)
  # =============================================================================
  chat-smart:
    description: "Best chat/reasoning models - ranked by Chatbot Arena and AI leaderboards"
    fallback_chain:
      # Tier 1: Flagship Models
      # DeepSeek V3.2
      - provider: nvidia
        model: "deepseek-ai/deepseek-v3.2"
        priority: 1
      - provider: together
        model: "deepseek-ai/DeepSeek-V3"
        priority: 1

      # Gemini 3 Pro
      - provider: gemini
        model: "gemini-3-pro-preview"
        priority: 2
      - provider: google
        model: "gemini-3-pro-preview"
        priority: 2

      # GPT-5.1
      - provider: openai
        model: "gpt-5.1"
        priority: 3

      # Tier 2: Free High Performance
      # Together MoA-1 Turbo (Mixture of Agents)
      - provider: together
        model: "togethercomputer/MoA-1-Turbo"
        priority: 4

      # Llama 3.3 70B (Groq)
      - provider: groq
        model: "llama-3.3-70b-versatile"
        priority: 5

      # Gemini 2.5 Pro
      - provider: gemini
        model: "gemini-2.5-pro"
        priority: 6

    settings:
      timeout_ms: 180000
      retry_on_rate_limit: true

  # =============================================================================
  # CHAT FAST (LOWEST LATENCY)
  # =============================================================================
  chat-fast:
    description: "Fastest chat models (Lowest Latency)"
    fallback_chain:
      # Gemini 2.0 Flash Lite
      - provider: gemini
        model: "gemini-2.0-flash-lite"
        priority: 1

      # Llama 3.1 8B Turbo
      - provider: together
        model: "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
        priority: 2

      # Groq Llama 3.1 8B
      - provider: groq
        model: "llama-3.1-8b-instant"
        priority: 3

      # Gemini 2.5 Flash
      - provider: gemini
        model: "gemini-2.5-flash"
        priority: 4

      # Gemma 3 4B
      - provider: gemini
        model: "gemma-3-4b-it"
        priority: 5

    settings:
      timeout_ms: 15000
      retry_on_rate_limit: true

  # =============================================================================
  # CHAT RP (Roleplay - UGI Leaderboard RP Models)
  # =============================================================================
  chat-rp:
    description: "Best RP models from UGI leaderboard"
    fallback_chain:
      # UGI TOP RANKED RP MODELS
      - provider: g4f
        model: "mn-violet-lotus-12b"
        priority: 1
      - provider: g4f
        model: "violet-twilight-v0.2"
        priority: 2
      - provider: g4f
        model: "captain-eris-violet-v0.420"
        priority: 3
      - provider: g4f
        model: "mn-12b-lyra-v4"
        priority: 4

      # FASTER RP OPTIONS
      - provider: cerebras
        model: "llama-3.1-8b"
        priority: 5
      - provider: groq
        model: "llama-3.1-8b-instant"
        priority: 6

    settings:
      timeout_ms: 30000
      retry_on_rate_limit: true

# =============================================================================
# PROVIDERS (Registered APIs)
# =============================================================================
providers:
  nvidia:
    base_url: "https://integrate.api.nvidia.com/v1"
    api_key_env: "NVIDIA_API_KEY"
    rate_limits: { rpm: 60 }

  together:
    base_url: "https://api.together.xyz/v1"
    api_key_env: "TOGETHER_API_KEY"
    rate_limits: { rpm: 60 }

  cerebras:
    base_url: "https://api.cerebras.ai/v1"
    api_key_env: "CEREBRAS_API_KEY"
    rate_limits: { rpm: 50, daily: 5000 }

  groq:
    base_url: "https://api.groq.com/openai/v1"
    api_key_env: "GROQ_API_KEY"
    rate_limits: { rpm: 30, daily: 14400 }

  google:
    base_url: "https://generativelanguage.googleapis.com/v1beta"
    api_key_env: "GEMINI_API_KEY"
    rate_limits: { rpm: 15, daily: 1500 }

  gemini:
    base_url: "https://generativelanguage.googleapis.com/v1beta"
    api_key_env: "GEMINI_API_KEY"
    rate_limits: { rpm: 15, daily: 1500 }

  openai:
    base_url: "https://api.openai.com/v1"
    api_key_env: "OPENAI_API_KEY"
    rate_limits: { rpm: 3000 }

  mistral:
    base_url: "https://api.mistral.ai/v1"
    api_key_env: "MISTRAL_API_KEY"
    rate_limits: { rpm: 10 }

  g4f:
    base_url: "https://g4f.dev/v1"
    api_key_env: "G4F_API_KEY"
    rate_limits: { rpm: 10, daily: 100 }

