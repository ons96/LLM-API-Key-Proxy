virtual_models:
  # =============================================================================
  # CODING ELITE / SMART (Top Agentic Coding Models)
  # =============================================================================
  # Strategy: Best Model -> All Free Providers (G4F/Puter/Others) -> Next Best Model
  # Source: LiveBench Agentic Coding
  coding-elite:
    description: "Top-tier agentic coding models (LiveBench ranked) using purely free providers"
    fallback_chain:
      # 1. GPT-4o (G4F) - Reliable
      - provider: g4f
        model: "gpt-4o"
        priority: 1
      
      # 2. GPT-4 (G4F) - Backup
      - provider: g4f
        model: "gpt-4"
        priority: 2
      
      # 3. Claude 3 Opus (G4F)
      - provider: g4f
        model: "claude-3-opus"
        priority: 3


    settings:
      timeout_ms: 180000
      retry_on_rate_limit: true

  coding-smart:
    description: "Best coding performance models (SWE-bench/LiveBench)"
    fallback_chain:
      - provider: anthropic
        model: "claude-3-5-sonnet-latest"
      - provider: openai
        model: "gpt-4o"
      - provider: g4f
        model: "gpt-4o"
    settings:
      timeout_ms: 120000

  coding-balanced:
    description: "Balanced coding models (Good quality + Moderate speed)"
    fallback_chain:
      - provider: groq
        model: "llama-3.3-70b-versatile"
      - provider: google
        model: "gemini-1.5-pro"
      - provider: anthropic
        model: "claude-3-5-haiku"
    settings:
      timeout_ms: 60000

  # =============================================================================
  # CODING FAST (Highest TPS for Code Edits/Greps)
  # =============================================================================
  # Source: Artificial Analysis (Cerebras, Groq, SambaNova)
  coding-fast:
    description: "Highest TPS coding models for fast edits and greps"
    fallback_chain:
      # 1. Cerebras: Llama 3.1 70B (~3000 t/s) - FASTEST
      - provider: cerebras
        model: "llama-3.1-70b"
        priority: 1
        free_tier_only: true
      
      # 2. Groq: Llama 3.3 70B (~1000 t/s)
      - provider: groq
        model: "llama-3.3-70b-versatile"
        priority: 2
        free_tier_only: true
      
      # 3. SambaNova: Llama 3.1 8B (~1000 t/s)
      - provider: sambanova
        model: "llama-3.1-8b"
        priority: 3
        free_tier_only: true
      
      # 4. Gemini 3 Flash (ZenMux/Google) - Very fast + big context
      - provider: zenmux
        model: "gemini-3-flash"
        priority: 4
      - provider: google
        model: "gemini-3-flash"
        priority: 4
        free_tier_only: true

      # 5. Grok Code Fast (OpenCode Zen)
      - provider: opencode_zen
        model: "grok-code-fast"
        priority: 5

    settings:
      timeout_ms: 30000
      retry_on_rate_limit: true

  # =============================================================================
  # CHAT SMART (Reasoning - formerly reasoning-elite)
  # =============================================================================
  chat-smart:
    description: "Best reasoning models for difficult questions"
    fallback_chain:
      # 1. Claude Opus 4.5 Thinking
      - provider: g4f
        model: "claude-opus-4-5-thinking"
        priority: 1
      
      # 2. GPT-5.2 High
      - provider: g4f
        model: "gpt-5.2-high"
        priority: 2
      
      # 3. Gemini 3 Pro
      - provider: google
        model: "gemini-3-pro"
        priority: 3
      
      # 4. DeepSeek R1 / V3.2 Thinking (Reasoning specialized)
      - provider: iflow
        model: "deepseek-reasoner"
        priority: 4
      - provider: deepseek
        model: "deepseek-reasoner"
        priority: 4
      
      # 5. Kimi K2 Thinking (Nvidia)
      - provider: nvidia
        model: "kimi-k2-thinking"
        priority: 5

    settings:
      timeout_ms: 180000

  # =============================================================================
  # CHAT FAST (Highest TPS Chat)
  # =============================================================================
  chat-fast:
    description: "Fastest chat models (Cerebras/Groq)"
    fallback_chain:
      # 1. Cerebras: Llama 3.1 8B (~3000 t/s)
      - provider: cerebras
        model: "llama-3.1-8b"
        priority: 1
        free_tier_only: true
      
      # 2. Groq: Llama 3.1 8B (~1000 t/s)
      - provider: groq
        model: "llama-3.1-8b-instant"
        priority: 2
        free_tier_only: true
      
      # 3. SambaNova: Llama 3.2 1B (Instant)
      - provider: sambanova
        model: "llama-3.2-1b"
        priority: 3
      
      # 4. MiMo V2 Flash (ZenMux)
      - provider: zenmux
        model: "mimo-v2-flash"
        priority: 4

    settings:
      timeout_ms: 15000

# =============================================================================
# PROVIDERS (New Free Providers Added)
# =============================================================================
providers:
  # --- New Aggregators ---
  zenmux:
    base_url: "https://api.zenmux.com/v1"
    api_key_env: "ZENMUX_API_KEY"
    rate_limits: { rpm: 20, daily: 500 }

  iflow:
    base_url: "https://api.iflow.com/v1"
    api_key_env: "IFLOW_API_KEY"
    rate_limits: { rpm: 60 }

  opencode_zen:
    base_url: "https://api.opencodezen.com/v1"
    api_key_env: "OPENCODE_ZEN_API_KEY"
    rate_limits: { rpm: 50 }

  aihubmix:
    base_url: "https://api.aihubmix.com/v1"
    api_key_env: "AIHUBMIX_API_KEY"
    rate_limits: { rpm: 50 }

  routeway:
    base_url: "https://routeway.ai/v1"
    api_key_env: "ROUTEWAY_API_KEY"
    rate_limits: { rpm: 20, daily: 200 }

  modelscope:
    base_url: "https://api-inference.modelscope.cn/v1"
    api_key_env: "MODELSCOPE_API_KEY"
    rate_limits: { rpm: 60 }

  # --- New Direct Providers ---
  nvidia:
    base_url: "https://integrate.api.nvidia.com/v1"
    api_key_env: "NVIDIA_API_KEY"
    rate_limits: { rpm: 60 }

  sambanova:
    base_url: "https://api.sambanova.ai/v1"
    api_key_env: "SAMBANOVA_API_KEY"
    rate_limits: { rpm: 60 }

  mistral:
    base_url: "https://api.mistral.ai/v1"
    api_key_env: "MISTRAL_API_KEY"
    rate_limits: { rpm: 10 }

  # --- Existing ---
  google:
    base_url: "https://generativelanguage.googleapis.com/v1beta"
    api_key_env: "GEMINI_API_KEY"
    rate_limits: { rpm: 15, daily: 1500 }

  groq:
    base_url: "https://api.groq.com/openai/v1"
    api_key_env: "GROQ_API_KEY"
    rate_limits: { rpm: 30, daily: 14400 }

  cerebras:
    base_url: "https://api.cerebras.ai/v1"
    api_key_env: "CEREBRAS_API_KEY"
    rate_limits: { rpm: 50, daily: 5000 }
  
  g4f:
    base_url: "https://g4f.dev/v1"
    api_key_env: "G4F_API_KEY"
    rate_limits: { rpm: 10, daily: 100 }

  puter:
    base_url: "https://puter.com/api/v1" # Needs adapter logic
    api_key_env: "PUTER_API_KEY"
    rate_limits: { rpm: 20 }
